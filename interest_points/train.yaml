# # caric_rl_config.yaml
# # CARIC-RL专用配置文件

# defaults:
#   - _self_

# seed: 42
# headless: true
# device: cuda:0

# # 最大训练帧数和评估间隔
# max_frame_num: 5000000  # 5M frames for thorough training
# eval_interval: 100      # 每100步评估一次（更频繁的评估）
# save_interval: 500      # 每500步保存模型

# # 早停机制（基于CARIC指标）
# early_stopping_patience: 30  # 30次评估无改进则停止

# # CARIC-RL环境配置
# env:
#   num_envs: 1  # 并行环境数量（减少以节省内存）
#   max_episode_length: 1500  # 最大episode长度
  
#   # CARIC核心参数
#   scan_distance: 4.0  # 扫描距离
#   exploration_threshold: 0.8  # 探索完成阈值
  
#   # 建筑环境配置
#   building_complexity: "simple"  # 建筑复杂度：simple/medium/complex
#   random_buildings: true  # 随机生成建筑
  
#   # 状态维度
#   state_dim: 12  # 基础状态维度

# # 传感器配置（针对CARIC优化）
# sensor:
#   # LiDAR配置（用于建图和障碍物检测，类似CARIC）
#   lidar_range: 25.0
#   lidar_vfov: [-30.0, 10.0]  # 垂直视场角
#   lidar_vbeams: 16  # 垂直光束数
#   lidar_hres: 2.0  # 水平分辨率（2度）
  
#   # 相机配置（固定在baselink，用于兴趣点扫描）
#   camera_fov: 60.0  # 相机视场角
#   camera_range: 15.0  # 相机检测距离

# # 无人机配置
# drone:
#   model_name: "Hummingbird"  # 无人机模型
  
# # 仿真配置
# sim:
#   dt: 0.02  # 时间步长
#   substeps: 1
#   render_interval: 8

# # CARIC-Aware PPO算法配置
# algo:
#   # 数据收集
#   training_frame_num: 200  # 每次训练的帧数（减少以适应复杂观察）
#   num_minibatches: 4
#   training_epoch_num: 8    # 减少epochs以加快训练
  
#   # 策略网络配置
#   actor:
#     learning_rate: 3e-4    # 稍高的学习率
#     clip_ratio: 0.2
    
#   # 价值网络配置  
#   critic:
#     learning_rate: 1e-3    # 更高的学习率用于价值学习
#     clip_ratio: 0.2
    
#   # 特征提取器配置
#   feature_extractor:
#     learning_rate: 1e-4    # 保守的特征学习率
    
#   # 损失系数
#   entropy_loss_coefficient: 0.02  # 稍高的熵系数鼓励探索

# # Wandb配置
# wandb:
#   project: CARIC-RL-Hybrid
#   name: building_training
#   entity: 3237739925-ntu
#   mode: online
#   run_id:
# # CARIC特定参数
# caric:
#   # 兴趣点管理
#   max_interest_points: 100
#   scan_completion_distance: 3.0
  
#   # 地图参数  
#   voxel_size: 0.5
#   map_range: [40.0, 40.0, 20.0]  # [x, y, z] 地图范围
  
#   # 检测阈值
#   edge_detection_threshold: 0.3
#   corner_detection_threshold: 0.5
  
#   # 奖励权重
#   reward_weights:
#     discovery: 2.0      # 发现兴趣点
#     scanning: 5.0       # 完成扫描（最重要）
#     exploration: 1.0    # 探索新区域
#     efficiency: 1.5     # 路径效率
#     target: 3.0         # 目标导向
#     collision: -10.0    # 碰撞惩罚

defaults:
  - _self_

seed: 42
headless: true
device: cuda:0

# Training schedule (inspired by navigation project)
max_frame_num: 2000000  # 2M frames for stable convergence
eval_interval: 200      # Frequent evaluation for early detection
save_interval: 500      
early_stopping_patience: 20

# Environment configuration (stability first)
env:
  num_envs: 2  # Small number for debugging and stable training
  max_episode_length: 1000  # Moderate episode length
  
  # CARIC core parameters
  scan_distance: 3.0
  exploration_threshold: 0.8
  
  # Reduced complexity for stability
  building_complexity: "simple"
  random_buildings: true
  state_dim: 12

# Sensor configuration (simplified like navigation project)
sensor:
  # Simplified LiDAR configuration
  lidar_range: 4.0
  lidar_vfov: [-10.0, 20.0]
  lidar_vbeams: 4
  lidar_hres: 10.0
  
  # Camera configuration
  camera_fov: 60.0
  camera_range: 10.0

# Drone configuration
drone:
  model_name: "Hummingbird"

# Simulation configuration (matched to navigation project)
sim:
  dt: 0.016  # 16ms timestep for stability
  substeps: 1
  render_interval: 8

# Improved PPO algorithm configuration (navigation project parameters)
algo:
  # Data collection (small batches for stability)
  training_frame_num: 32  # Navigation project batch size
  num_minibatches: 16     # Navigation project minibatches
  training_epoch_num: 4   # Navigation project epochs
  
  # Feature extractor configuration
  feature_extractor:
    learning_rate: 5e-4   # Navigation project learning rate
    dyn_obs_num: 5
    
  # Actor configuration (conservative parameters)
  actor:
    learning_rate: 5e-4   # Reduced from 3e-4
    clip_ratio: 0.1       # Conservative clipping (navigation project)
    action_limit: 2.0     # Bounded actions (navigation project)
    
  # Critic configuration
  critic:
    learning_rate: 5e-4   # Consistent with actor
    clip_ratio: 0.1       # Conservative clipping
    
  # Loss coefficients (small entropy for stability)
  entropy_loss_coefficient: 1e-3  # Navigation project entropy coefficient

# Wandb configuration
wandb:
  project: CARIC-RL-Stable
  name: stable_beta_training
  entity: 3237739925-ntu
  mode: online
  run_id:

# CARIC specific parameters (balanced rewards)
caric:
  # Interest point management (reduced complexity)
  max_interest_points: 50
  scan_completion_distance: 2.5
  
  # Map parameters (smaller for stability)
  voxel_size: 0.5
  map_range: [20.0, 20.0, 10.0]
  
  # Detection thresholds
  edge_detection_threshold: 0.3
  corner_detection_threshold: 0.5
  
  # Balanced reward weights (inspired by navigation project balance)
  reward_weights:
    base: 0.1           # Base survival reward
    target: 1.0         # Target guidance (main driver)
    discovery: 0.5      # Discovery (reduced from 2.0)
    scanning: 1.5       # Scanning completion (reduced from 5.0)
    exploration: 0.3    # Exploration (reduced from 1.0)
    smoothness: 0.5     # Flight smoothness (new)
    safety: 0.8         # Safety (important but not dominant)
    height: 0.3         # Height reasonableness (small weight)
    # Action smoothing parameters
action_smoothing:
  enabled: true
  smoothing_coefficient: 0.7  # How much to smooth (0=no smooth, 1=no change)
  max_change_per_step: 1.0    # Maximum action change per timestep